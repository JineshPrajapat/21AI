import React from "react";

const data =`<p class="demoTitle">&nbsp; &nbsp;</p>
<p>I will assume a basic understanding of neural networks and backpropagation. If you&rsquo;d like to brush up,&nbsp;<a href="https://mlvu.github.io/lecture06/">this lecture</a>&nbsp;will give you the basics of neural networks and&nbsp;<a href="https://mlvu.github.io/lecture07/">this one</a>&nbsp;will explain how these principles are applied in modern deep learning systems.</p>
<p>A&nbsp;<a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">working knowledge of Pytorch</a>&nbsp;is required to understand the programming examples, but these can also be safely skipped.</p>
<h2 id="self-attention"><span style="color: #ffcc00;">Self-attention</span></h2>
<p>The fundamental operation of any transformer architecture is the&nbsp;<em>self-attention operation</em>.</p>
<aside>We'll explain where the name "self-attention" comes from later. For now, don't read too much in to it.</aside>
<p>Self-attention is a sequence-to-sequence operation: a sequence of vectors goes in, and a sequence of vectors comes out. Let&rsquo;s call the input vectors&nbsp;<span id="MathJax-Element-1-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-1" class="math"><span id="MathJax-Span-2" class="mrow"><span id="MathJax-Span-3" class="msubsup"><span id="MathJax-Span-4" class="texatom"><span id="MathJax-Span-5" class="mrow"><span id="MathJax-Span-6" class="mi">𝐱</span></span></span><span id="MathJax-Span-7" class="mn">1</span></span><span id="MathJax-Span-8" class="mo">,</span><span id="MathJax-Span-9" class="msubsup"><span id="MathJax-Span-10" class="texatom"><span id="MathJax-Span-11" class="mrow"><span id="MathJax-Span-12" class="mi">𝐱</span></span></span><span id="MathJax-Span-13" class="mn">2</span></span><span id="MathJax-Span-14" class="mo">,</span><span id="MathJax-Span-15" class="mo">&hellip;</span><span id="MathJax-Span-16" class="mo">,</span><span id="MathJax-Span-17" class="msubsup"><span id="MathJax-Span-18" class="texatom"><span id="MathJax-Span-19" class="mrow"><span id="MathJax-Span-20" class="mi">𝐱</span></span></span><span id="MathJax-Span-21" class="mi">t</span></span></span></span></span>&nbsp;and the corresponding output vectors&nbsp;<span id="MathJax-Element-2-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-22" class="math"><span id="MathJax-Span-23" class="mrow"><span id="MathJax-Span-24" class="msubsup"><span id="MathJax-Span-25" class="texatom"><span id="MathJax-Span-26" class="mrow"><span id="MathJax-Span-27" class="mi">𝐲</span></span></span><span id="MathJax-Span-28" class="mn">1</span></span><span id="MathJax-Span-29" class="mo">,</span><span id="MathJax-Span-30" class="msubsup"><span id="MathJax-Span-31" class="texatom"><span id="MathJax-Span-32" class="mrow"><span id="MathJax-Span-33" class="mi">𝐲</span></span></span><span id="MathJax-Span-34" class="mn">2</span></span><span id="MathJax-Span-35" class="mo">,</span><span id="MathJax-Span-36" class="mo">&hellip;</span><span id="MathJax-Span-37" class="mo">,</span><span id="MathJax-Span-38" class="msubsup"><span id="MathJax-Span-39" class="texatom"><span id="MathJax-Span-40" class="mrow"><span id="MathJax-Span-41" class="mi">𝐲</span></span></span><span id="MathJax-Span-42" class="mi">t</span></span></span></span></span>. The vectors all have dimension&nbsp;<span id="MathJax-Element-3-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-43" class="math"><span id="MathJax-Span-44" class="mrow"><span id="MathJax-Span-45" class="mi">k</span></span></span></span>.</p>
<p>To produce output vector&nbsp;<span id="MathJax-Element-4-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-46" class="math"><span id="MathJax-Span-47" class="mrow"><span id="MathJax-Span-48" class="msubsup"><span id="MathJax-Span-49" class="texatom"><span id="MathJax-Span-50" class="mrow"><span id="MathJax-Span-51" class="mi">𝐲</span></span></span><span id="MathJax-Span-52" class="texatom"><span id="MathJax-Span-53" class="mrow"><span id="MathJax-Span-54" class="mstyle"><span id="MathJax-Span-55" class="mrow"><span id="MathJax-Span-56" class="mi">i</span></span></span></span></span></span></span></span></span>, the self attention operation simply takes&nbsp;<em>a weighted average over all the input vectors</em></p>
<div class="MathJax_Display"><span id="MathJax-Element-5-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-57" class="math"><span id="MathJax-Span-58" class="mrow"><span id="MathJax-Span-59" class="msubsup"><span id="MathJax-Span-60" class="texatom"><span id="MathJax-Span-61" class="mrow"><span id="MathJax-Span-62" class="mi">𝐲</span></span></span><span id="MathJax-Span-63" class="texatom"><span id="MathJax-Span-64" class="mrow"><span id="MathJax-Span-65" class="mstyle"><span id="MathJax-Span-66" class="mrow"><span id="MathJax-Span-67" class="mi">i</span></span></span></span></span></span><span id="MathJax-Span-68" class="mo">=</span><span id="MathJax-Span-69" class="munderover"><span id="MathJax-Span-70" class="mo">&sum;</span><span id="MathJax-Span-71" class="texatom"><span id="MathJax-Span-72" class="mrow"><span id="MathJax-Span-73" class="texatom"><span id="MathJax-Span-74" class="mrow"><span id="MathJax-Span-75" class="mstyle"><span id="MathJax-Span-76" class="mrow"><span id="MathJax-Span-77" class="mi">j</span></span></span></span></span></span></span></span><span id="MathJax-Span-78" class="msubsup"><span id="MathJax-Span-79" class="mi">w</span><span id="MathJax-Span-80" class="texatom"><span id="MathJax-Span-81" class="mrow"><span id="MathJax-Span-82" class="texatom"><span id="MathJax-Span-83" class="mrow"><span id="MathJax-Span-84" class="mstyle"><span id="MathJax-Span-85" class="mrow"><span id="MathJax-Span-86" class="mi">i</span></span></span></span></span><span id="MathJax-Span-87" class="texatom"><span id="MathJax-Span-88" class="mrow"><span id="MathJax-Span-89" class="mstyle"><span id="MathJax-Span-90" class="mrow"><span id="MathJax-Span-91" class="mi">j</span></span></span></span></span></span></span></span><span id="MathJax-Span-92" class="msubsup"><span id="MathJax-Span-93" class="texatom"><span id="MathJax-Span-94" class="mrow"><span id="MathJax-Span-95" class="mi">𝐱</span></span></span><span id="MathJax-Span-96" class="texatom"><span id="MathJax-Span-97" class="mrow"><span id="MathJax-Span-98" class="mstyle"><span id="MathJax-Span-99" class="mrow"><span id="MathJax-Span-100" class="mi">j</span></span></span></span></span></span><span id="MathJax-Span-101" class="mspace"></span><span id="MathJax-Span-102" class="mtext">.</span></span></span></span></div>
<p>Where&nbsp;<span id="MathJax-Element-6-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-103" class="math"><span id="MathJax-Span-104" class="mrow"><span id="MathJax-Span-105" class="texatom"><span id="MathJax-Span-106" class="mrow"><span id="MathJax-Span-107" class="mstyle"><span id="MathJax-Span-108" class="mrow"><span id="MathJax-Span-109" class="mi">j</span></span></span></span></span></span></span></span>&nbsp;indexes over the whole sequence and the weights sum to one over all&nbsp;<span id="MathJax-Element-7-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-110" class="math"><span id="MathJax-Span-111" class="mrow"><span id="MathJax-Span-112" class="texatom"><span id="MathJax-Span-113" class="mrow"><span id="MathJax-Span-114" class="mstyle"><span id="MathJax-Span-115" class="mrow"><span id="MathJax-Span-116" class="mi">j</span></span></span></span></span></span></span></span>. The weight&nbsp;<span id="MathJax-Element-8-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-117" class="math"><span id="MathJax-Span-118" class="mrow"><span id="MathJax-Span-119" class="msubsup"><span id="MathJax-Span-120" class="mi">w</span><span id="MathJax-Span-121" class="texatom"><span id="MathJax-Span-122" class="mrow"><span id="MathJax-Span-123" class="texatom"><span id="MathJax-Span-124" class="mrow"><span id="MathJax-Span-125" class="mstyle"><span id="MathJax-Span-126" class="mrow"><span id="MathJax-Span-127" class="mi">i</span></span></span></span></span><span id="MathJax-Span-128" class="texatom"><span id="MathJax-Span-129" class="mrow"><span id="MathJax-Span-130" class="mstyle"><span id="MathJax-Span-131" class="mrow"><span id="MathJax-Span-132" class="mi">j</span></span></span></span></span></span></span></span></span></span></span>&nbsp;is not a parameter, as in a normal neural net, but it is&nbsp;<em>derived</em>&nbsp;from a function over&nbsp;<span id="MathJax-Element-9-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-133" class="math"><span id="MathJax-Span-134" class="mrow"><span id="MathJax-Span-135" class="msubsup"><span id="MathJax-Span-136" class="texatom"><span id="MathJax-Span-137" class="mrow"><span id="MathJax-Span-138" class="mi">𝐱</span></span></span><span id="MathJax-Span-139" class="texatom"><span id="MathJax-Span-140" class="mrow"><span id="MathJax-Span-141" class="mstyle"><span id="MathJax-Span-142" class="mrow"><span id="MathJax-Span-143" class="mi">i</span></span></span></span></span></span></span></span></span>&nbsp;and&nbsp;<span id="MathJax-Element-10-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-144" class="math"><span id="MathJax-Span-145" class="mrow"><span id="MathJax-Span-146" class="msubsup"><span id="MathJax-Span-147" class="texatom"><span id="MathJax-Span-148" class="mrow"><span id="MathJax-Span-149" class="mi">𝐱</span></span></span><span id="MathJax-Span-150" class="texatom"><span id="MathJax-Span-151" class="mrow"><span id="MathJax-Span-152" class="mstyle"><span id="MathJax-Span-153" class="mrow"><span id="MathJax-Span-154" class="mi">j</span></span></span></span></span></span></span></span></span>. The simplest option for this function is the dot product:</p>
<div class="MathJax_Display"><span id="MathJax-Element-11-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-155" class="math"><span id="MathJax-Span-156" class="mrow"><span id="MathJax-Span-157" class="msubsup"><span id="MathJax-Span-158" class="mi">w</span><span id="MathJax-Span-159" class="mo">&prime;</span><span id="MathJax-Span-160" class="texatom"><span id="MathJax-Span-161" class="mrow"><span id="MathJax-Span-162" class="texatom"><span id="MathJax-Span-163" class="mrow"><span id="MathJax-Span-164" class="mstyle"><span id="MathJax-Span-165" class="mrow"><span id="MathJax-Span-166" class="mi">i</span></span></span></span></span><span id="MathJax-Span-167" class="texatom"><span id="MathJax-Span-168" class="mrow"><span id="MathJax-Span-169" class="mstyle"><span id="MathJax-Span-170" class="mrow"><span id="MathJax-Span-171" class="mi">j</span></span></span></span></span></span></span></span><span id="MathJax-Span-172" class="mo">=</span><span id="MathJax-Span-173" class="msubsup"><span id="MathJax-Span-174" class="texatom"><span id="MathJax-Span-175" class="mrow"><span id="MathJax-Span-176" class="msubsup"><span id="MathJax-Span-177" class="texatom"><span id="MathJax-Span-178" class="mrow"><span id="MathJax-Span-179" class="mi">𝐱</span></span></span><span id="MathJax-Span-180" class="texatom"><span id="MathJax-Span-181" class="mrow"><span id="MathJax-Span-182" class="mstyle"><span id="MathJax-Span-183" class="mrow"><span id="MathJax-Span-184" class="mi">i</span></span></span></span></span></span></span></span><span id="MathJax-Span-185" class="mi">T</span></span><span id="MathJax-Span-186" class="msubsup"><span id="MathJax-Span-187" class="texatom"><span id="MathJax-Span-188" class="mrow"><span id="MathJax-Span-189" class="mi">𝐱</span></span></span><span id="MathJax-Span-190" class="texatom"><span id="MathJax-Span-191" class="mrow"><span id="MathJax-Span-192" class="mstyle"><span id="MathJax-Span-193" class="mrow"><span id="MathJax-Span-194" class="mi">j</span></span></span></span></span></span><span id="MathJax-Span-195" class="mspace"></span><span id="MathJax-Span-196" class="mtext">.</span></span></span></span></div>
<aside>Note that&nbsp;<span id="MathJax-Element-12-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-197" class="math"><span id="MathJax-Span-198" class="mrow"><span id="MathJax-Span-199" class="msubsup"><span id="MathJax-Span-200" class="texatom"><span id="MathJax-Span-201" class="mrow"><span id="MathJax-Span-202" class="mi">𝐱</span></span></span><span id="MathJax-Span-203" class="texatom"><span id="MathJax-Span-204" class="mrow"><span id="MathJax-Span-205" class="mstyle"><span id="MathJax-Span-206" class="mrow"><span id="MathJax-Span-207" class="mi">i</span></span></span></span></span></span></span></span></span>&nbsp;is the input vector at the same position as the current output vector&nbsp;<span id="MathJax-Element-13-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-208" class="math"><span id="MathJax-Span-209" class="mrow"><span id="MathJax-Span-210" class="msubsup"><span id="MathJax-Span-211" class="texatom"><span id="MathJax-Span-212" class="mrow"><span id="MathJax-Span-213" class="mi">𝐲</span></span></span><span id="MathJax-Span-214" class="texatom"><span id="MathJax-Span-215" class="mrow"><span id="MathJax-Span-216" class="mstyle"><span id="MathJax-Span-217" class="mrow"><span id="MathJax-Span-218" class="mi">i</span></span></span></span></span></span></span></span></span>. For the next output vector, we get an entirely new series of dot products, and a different weighted sum.</aside>
<p>The dot product gives us a value anywhere between negative and positive infinity, so we apply a softmax to map the values to&nbsp;<span id="MathJax-Element-14-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-219" class="math"><span id="MathJax-Span-220" class="mrow"><span id="MathJax-Span-221" class="mo">[</span><span id="MathJax-Span-222" class="mn">0</span><span id="MathJax-Span-223" class="mo">,</span><span id="MathJax-Span-224" class="mn">1</span><span id="MathJax-Span-225" class="mo">]</span></span></span></span>&nbsp;and to ensure that they sum to 1 over the whole sequence:</p>
<div class="MathJax_Display"><span id="MathJax-Element-15-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-226" class="math"><span id="MathJax-Span-227" class="mrow"><span id="MathJax-Span-228" class="msubsup"><span id="MathJax-Span-229" class="mi">w</span><span id="MathJax-Span-230" class="texatom"><span id="MathJax-Span-231" class="mrow"><span id="MathJax-Span-232" class="texatom"><span id="MathJax-Span-233" class="mrow"><span id="MathJax-Span-234" class="mstyle"><span id="MathJax-Span-235" class="mrow"><span id="MathJax-Span-236" class="mi">i</span></span></span></span></span><span id="MathJax-Span-237" class="texatom"><span id="MathJax-Span-238" class="mrow"><span id="MathJax-Span-239" class="mstyle"><span id="MathJax-Span-240" class="mrow"><span id="MathJax-Span-241" class="mi">j</span></span></span></span></span></span></span></span><span id="MathJax-Span-242" class="mo">=</span><span id="MathJax-Span-243" class="mfrac"><span id="MathJax-Span-244" class="mrow"><span id="MathJax-Span-245" class="mtext">exp&nbsp;</span><span id="MathJax-Span-246" class="msubsup"><span id="MathJax-Span-247" class="mi">w</span><span id="MathJax-Span-248" class="mo">&prime;</span><span id="MathJax-Span-249" class="texatom"><span id="MathJax-Span-250" class="mrow"><span id="MathJax-Span-251" class="texatom"><span id="MathJax-Span-252" class="mrow"><span id="MathJax-Span-253" class="mstyle"><span id="MathJax-Span-254" class="mrow"><span id="MathJax-Span-255" class="mi">i</span></span></span></span></span><span id="MathJax-Span-256" class="texatom"><span id="MathJax-Span-257" class="mrow"><span id="MathJax-Span-258" class="mstyle"><span id="MathJax-Span-259" class="mrow"><span id="MathJax-Span-260" class="mi">j</span></span></span></span></span></span></span></span></span><span id="MathJax-Span-261" class="mrow"><span id="MathJax-Span-262" class="munderover"><span id="MathJax-Span-263" class="mo">&sum;</span><span id="MathJax-Span-264" class="texatom"><span id="MathJax-Span-265" class="mrow"><span id="MathJax-Span-266" class="mstyle"><span id="MathJax-Span-267" class="mrow"><span id="MathJax-Span-268" class="mi">j</span></span></span></span></span></span><span id="MathJax-Span-269" class="mtext">exp&nbsp;</span><span id="MathJax-Span-270" class="msubsup"><span id="MathJax-Span-271" class="mi">w</span><span id="MathJax-Span-272" class="mo">&prime;</span><span id="MathJax-Span-273" class="texatom"><span id="MathJax-Span-274" class="mrow"><span id="MathJax-Span-275" class="texatom"><span id="MathJax-Span-276" class="mrow"><span id="MathJax-Span-277" class="mstyle"><span id="MathJax-Span-278" class="mrow"><span id="MathJax-Span-279" class="mi">i</span></span></span></span></span><span id="MathJax-Span-280" class="texatom"><span id="MathJax-Span-281" class="mrow"><span id="MathJax-Span-282" class="mstyle"><span id="MathJax-Span-283" class="mrow"><span id="MathJax-Span-284" class="mi">j</span></span></span></span></span></span></span></span></span></span><span id="MathJax-Span-285" class="mspace"></span><span id="MathJax-Span-286" class="mtext">.</span></span></span></span></div>
<p>And that&rsquo;s the basic operation of self attention.</p>
<figure class="narrow"><img src="https://peterbloem.nl/files/transformers/self-attention.svg" alt="" />
<figcaption>A visual illustration of basic self-attention. Note that the softmax operation over the&nbsp;<span class="rc">weights</span>&nbsp;is not illustrated.</figcaption>
</figure>
<p>A few other ingredients are needed for a complete transformer, which we&rsquo;ll discuss later, but this is the fundamental operation. More importantly, this is the only operation in the whole architecture that propagates information&nbsp;<em>between</em>&nbsp;vectors. Every other operation in the transformer is applied to each vector in the input sequence without interactions between vectors.</p>
<h3 id="understanding-why-self-attention-works">Understanding why self-attention works</h3>
<p>Despite its simplicity, it&rsquo;s not immediately obvious why self-attention should work so well. To build up some intuition, let&rsquo;s look first at the standard approach to&nbsp;<em>movie recommendation</em>.</p>
<p>Let&rsquo;s say you run a movie rental business and you have some movies, and some users, and you would like to recommend movies to your users that they are likely to enjoy.</p>
<p>One way to go about this, is to create manual features for your movies, such as how much romance there is in the movie, and how much action, and then to design corresponding features for your users: how much they enjoy romantic movies and how much they enjoy action-based movies. If you did this, the dot product between the two feature vectors would give you a score for how well the attributes of the movie match what the user enjoys.</p>
<figure class="narrow"><img src="https://peterbloem.nl/files/transformers/dot-product.svg" alt="" /></figure>
<p>If the signs of a feature match for the user and the movie&mdash;the movie is romantic and the user loves romance or the movie is&nbsp;<em>unromantic</em>&nbsp;and the user hates romance&mdash;then the resulting dot product gets a positive term for that feature. If the signs don&rsquo;t match&mdash;the movie is romantic and the user hates romance or vice versa&mdash;the corresponding term is negative.</p>
<p>Furthermore, the&nbsp;<em>magnitudes</em>&nbsp;of the features indicate how much the feature should contribute to the total score: a movie may be a little romantic, but not in a noticeable way, or a user may simply prefer no romance, but be largely ambivalent.</p>
<p>Of course, gathering such features is not practical. Annotating a database of millions of movies is very costly, and annotating users with their likes and dislikes is pretty much impossible.</p>
<p>What happens instead is that we make the movie features and user features&nbsp;<em>parameters</em>&nbsp;of the model. We then ask users for a small number of movies that they like and we optimize the user features and movie features so that their dot product matches the known likes.</p>
<p>Even though we don&rsquo;t tell the model what any of the features should mean, in practice, it turns out that after training the features do actually reflect meaningful semantics about the movie content.</p>
<figure class="narrow"><img src="https://peterbloem.nl/files/transformers/movie-features.svg" alt="" />
<figcaption>The first two learned features from a basic matrix factorization model. The model had no access to any information about the content of the movies, only which users liked them. Note that movies are arranged from low-brow to high-brow horizontally, and from mainstream to quirky vertically. From [4].</figcaption>
</figure>
<aside>See&nbsp;<a href="https://mlvu.github.io/lecture12/">this lecture</a>&nbsp;for more details on recommender systems. For now, this suffices as an explanation of how the dot product helps us to represent objects and their relations.</aside>
<p>This is the basic principle at work in the self-attention. Let&rsquo;s say we are faced with a sequence of words. To apply self-attention, we simply assign each word&nbsp;<span id="MathJax-Element-16-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-287" class="math"><span id="MathJax-Span-288" class="mrow"><span id="MathJax-Span-289" class="texatom"><span id="MathJax-Span-290" class="mrow"><span id="MathJax-Span-291" class="mstyle"><span id="MathJax-Span-292" class="mrow"><span id="MathJax-Span-293" class="mi">t</span></span></span></span></span></span></span></span>&nbsp;in our vocabulary an&nbsp;<em>embedding vector</em>&nbsp;<span id="MathJax-Element-17-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-294" class="math"><span id="MathJax-Span-295" class="mrow"><span id="MathJax-Span-296" class="msubsup"><span id="MathJax-Span-297" class="texatom"><span id="MathJax-Span-298" class="mrow"><span id="MathJax-Span-299" class="mi">𝐯</span></span></span><span id="MathJax-Span-300" class="texatom"><span id="MathJax-Span-301" class="mrow"><span id="MathJax-Span-302" class="mstyle"><span id="MathJax-Span-303" class="mrow"><span id="MathJax-Span-304" class="mi">t</span></span></span></span></span></span></span></span></span>&nbsp;(the values of which we&rsquo;ll learn). This is what&rsquo;s known as an&nbsp;<em>embedding layer</em>&nbsp;in sequence modeling. It turns the word sequence&nbsp;<span id="MathJax-Element-18-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-305" class="math"><span id="MathJax-Span-306" class="mrow"><span id="MathJax-Span-307" class="texatom"><span id="MathJax-Span-308" class="mrow"><span id="MathJax-Span-309" class="mstyle"><span id="MathJax-Span-310" class="mrow"><span id="MathJax-Span-311" class="mtext">the</span></span></span></span></span><span id="MathJax-Span-312" class="mo">,</span><span id="MathJax-Span-313" class="texatom"><span id="MathJax-Span-314" class="mrow"><span id="MathJax-Span-315" class="mstyle"><span id="MathJax-Span-316" class="mrow"><span id="MathJax-Span-317" class="mtext">cat</span></span></span></span></span><span id="MathJax-Span-318" class="mo">,</span><span id="MathJax-Span-319" class="texatom"><span id="MathJax-Span-320" class="mrow"><span id="MathJax-Span-321" class="mstyle"><span id="MathJax-Span-322" class="mrow"><span id="MathJax-Span-323" class="mtext">walks</span></span></span></span></span><span id="MathJax-Span-324" class="mo">,</span><span id="MathJax-Span-325" class="texatom"><span id="MathJax-Span-326" class="mrow"><span id="MathJax-Span-327" class="mstyle"><span id="MathJax-Span-328" class="mrow"><span id="MathJax-Span-329" class="mtext">on</span></span></span></span></span><span id="MathJax-Span-330" class="mo">,</span><span id="MathJax-Span-331" class="texatom"><span id="MathJax-Span-332" class="mrow"><span id="MathJax-Span-333" class="mstyle"><span id="MathJax-Span-334" class="mrow"><span id="MathJax-Span-335" class="mtext">the</span></span></span></span></span><span id="MathJax-Span-336" class="mo">,</span><span id="MathJax-Span-337" class="texatom"><span id="MathJax-Span-338" class="mrow"><span id="MathJax-Span-339" class="mstyle"><span id="MathJax-Span-340" class="mrow"><span id="MathJax-Span-341" class="mtext">street</span></span></span></span></span></span></span></span>&nbsp;into the vector sequence</p>
<div class="MathJax_Display"><span id="MathJax-Element-19-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-342" class="math"><span id="MathJax-Span-343" class="mrow"><span id="MathJax-Span-344" class="msubsup"><span id="MathJax-Span-345" class="texatom"><span id="MathJax-Span-346" class="mrow"><span id="MathJax-Span-347" class="mi">𝐯</span></span></span><span id="MathJax-Span-348" class="texatom"><span id="MathJax-Span-349" class="mrow"><span id="MathJax-Span-350" class="mstyle"><span id="MathJax-Span-351" class="mrow"><span id="MathJax-Span-352" class="mtext">the</span></span></span></span></span></span><span id="MathJax-Span-353" class="mo">,</span><span id="MathJax-Span-354" class="msubsup"><span id="MathJax-Span-355" class="texatom"><span id="MathJax-Span-356" class="mrow"><span id="MathJax-Span-357" class="mi">𝐯</span></span></span><span id="MathJax-Span-358" class="texatom"><span id="MathJax-Span-359" class="mrow"><span id="MathJax-Span-360" class="mstyle"><span id="MathJax-Span-361" class="mrow"><span id="MathJax-Span-362" class="mtext">cat</span></span></span></span></span></span><span id="MathJax-Span-363" class="mo">,</span><span id="MathJax-Span-364" class="msubsup"><span id="MathJax-Span-365" class="texatom"><span id="MathJax-Span-366" class="mrow"><span id="MathJax-Span-367" class="mi">𝐯</span></span></span><span id="MathJax-Span-368" class="texatom"><span id="MathJax-Span-369" class="mrow"><span id="MathJax-Span-370" class="mstyle"><span id="MathJax-Span-371" class="mrow"><span id="MathJax-Span-372" class="mtext">walks</span></span></span></span></span></span><span id="MathJax-Span-373" class="mo">,</span><span id="MathJax-Span-374" class="msubsup"><span id="MathJax-Span-375" class="texatom"><span id="MathJax-Span-376" class="mrow"><span id="MathJax-Span-377" class="mi">𝐯</span></span></span><span id="MathJax-Span-378" class="texatom"><span id="MathJax-Span-379" class="mrow"><span id="MathJax-Span-380" class="mstyle"><span id="MathJax-Span-381" class="mrow"><span id="MathJax-Span-382" class="mtext">on</span></span></span></span></span></span><span id="MathJax-Span-383" class="mo">,</span><span id="MathJax-Span-384" class="msubsup"><span id="MathJax-Span-385" class="texatom"><span id="MathJax-Span-386" class="mrow"><span id="MathJax-Span-387" class="mi">𝐯</span></span></span><span id="MathJax-Span-388" class="texatom"><span id="MathJax-Span-389" class="mrow"><span id="MathJax-Span-390" class="mstyle"><span id="MathJax-Span-391" class="mrow"><span id="MathJax-Span-392" class="mtext">the</span></span></span></span></span></span><span id="MathJax-Span-393" class="mo">,</span><span id="MathJax-Span-394" class="msubsup"><span id="MathJax-Span-395" class="texatom"><span id="MathJax-Span-396" class="mrow"><span id="MathJax-Span-397" class="mi">𝐯</span></span></span><span id="MathJax-Span-398" class="texatom"><span id="MathJax-Span-399" class="mrow"><span id="MathJax-Span-400" class="mstyle"><span id="MathJax-Span-401" class="mrow"><span id="MathJax-Span-402" class="mtext">street</span></span></span></span></span></span><span id="MathJax-Span-403" class="mspace"></span><span id="MathJax-Span-404" class="mtext">.</span></span></span></span></div>
<p>If we feed this sequence into a self-attention layer, the output is another sequence of vectors</p>
<div class="MathJax_Display"><span id="MathJax-Element-20-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-405" class="math"><span id="MathJax-Span-406" class="mrow"><span id="MathJax-Span-407" class="msubsup"><span id="MathJax-Span-408" class="texatom"><span id="MathJax-Span-409" class="mrow"><span id="MathJax-Span-410" class="mi">𝐲</span></span></span><span id="MathJax-Span-411" class="texatom"><span id="MathJax-Span-412" class="mrow"><span id="MathJax-Span-413" class="mstyle"><span id="MathJax-Span-414" class="mrow"><span id="MathJax-Span-415" class="mtext">the</span></span></span></span></span></span><span id="MathJax-Span-416" class="mo">,</span><span id="MathJax-Span-417" class="msubsup"><span id="MathJax-Span-418" class="texatom"><span id="MathJax-Span-419" class="mrow"><span id="MathJax-Span-420" class="mi">𝐲</span></span></span><span id="MathJax-Span-421" class="texatom"><span id="MathJax-Span-422" class="mrow"><span id="MathJax-Span-423" class="mstyle"><span id="MathJax-Span-424" class="mrow"><span id="MathJax-Span-425" class="mtext">cat</span></span></span></span></span></span><span id="MathJax-Span-426" class="mo">,</span><span id="MathJax-Span-427" class="msubsup"><span id="MathJax-Span-428" class="texatom"><span id="MathJax-Span-429" class="mrow"><span id="MathJax-Span-430" class="mi">𝐲</span></span></span><span id="MathJax-Span-431" class="texatom"><span id="MathJax-Span-432" class="mrow"><span id="MathJax-Span-433" class="mstyle"><span id="MathJax-Span-434" class="mrow"><span id="MathJax-Span-435" class="mtext">walks</span></span></span></span></span></span><span id="MathJax-Span-436" class="mo">,</span><span id="MathJax-Span-437" class="msubsup"><span id="MathJax-Span-438" class="texatom"><span id="MathJax-Span-439" class="mrow"><span id="MathJax-Span-440" class="mi">𝐲</span></span></span><span id="MathJax-Span-441" class="texatom"><span id="MathJax-Span-442" class="mrow"><span id="MathJax-Span-443" class="mstyle"><span id="MathJax-Span-444" class="mrow"><span id="MathJax-Span-445" class="mtext">on</span></span></span></span></span></span><span id="MathJax-Span-446" class="mo">,</span><span id="MathJax-Span-447" class="msubsup"><span id="MathJax-Span-448" class="texatom"><span id="MathJax-Span-449" class="mrow"><span id="MathJax-Span-450" class="mi">𝐲</span></span></span><span id="MathJax-Span-451" class="texatom"><span id="MathJax-Span-452" class="mrow"><span id="MathJax-Span-453" class="mstyle"><span id="MathJax-Span-454" class="mrow"><span id="MathJax-Span-455" class="mtext">the</span></span></span></span></span></span><span id="MathJax-Span-456" class="mo">,</span><span id="MathJax-Span-457" class="msubsup"><span id="MathJax-Span-458" class="texatom"><span id="MathJax-Span-459" class="mrow"><span id="MathJax-Span-460" class="mi">𝐲</span></span></span><span id="MathJax-Span-461" class="texatom"><span id="MathJax-Span-462" class="mrow"><span id="MathJax-Span-463" class="mstyle"><span id="MathJax-Span-464" class="mrow"><span id="MathJax-Span-465" class="mtext">street</span></span></span></span></span></span></span></span></span></div>
<p>where&nbsp;<span id="MathJax-Element-21-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-466" class="math"><span id="MathJax-Span-467" class="mrow"><span id="MathJax-Span-468" class="msubsup"><span id="MathJax-Span-469" class="texatom"><span id="MathJax-Span-470" class="mrow"><span id="MathJax-Span-471" class="mi">𝐲</span></span></span><span id="MathJax-Span-472" class="texatom"><span id="MathJax-Span-473" class="mrow"><span id="MathJax-Span-474" class="mstyle"><span id="MathJax-Span-475" class="mrow"><span id="MathJax-Span-476" class="mtext">cat</span></span></span></span></span></span></span></span></span>&nbsp;is a weighted sum over all the embedding vectors in the first sequence, weighted by their (normalized) dot-product with&nbsp;<span id="MathJax-Element-22-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-477" class="math"><span id="MathJax-Span-478" class="mrow"><span id="MathJax-Span-479" class="msubsup"><span id="MathJax-Span-480" class="texatom"><span id="MathJax-Span-481" class="mrow"><span id="MathJax-Span-482" class="mi">𝐯</span></span></span><span id="MathJax-Span-483" class="texatom"><span id="MathJax-Span-484" class="mrow"><span id="MathJax-Span-485" class="mstyle"><span id="MathJax-Span-486" class="mrow"><span id="MathJax-Span-487" class="mtext">cat</span></span></span></span></span></span></span></span></span>.</p>
<p>Since we are&nbsp;<em>learning</em>&nbsp;what the values in&nbsp;<span id="MathJax-Element-23-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-488" class="math"><span id="MathJax-Span-489" class="mrow"><span id="MathJax-Span-490" class="msubsup"><span id="MathJax-Span-491" class="texatom"><span id="MathJax-Span-492" class="mrow"><span id="MathJax-Span-493" class="mi">𝐯</span></span></span><span id="MathJax-Span-494" class="texatom"><span id="MathJax-Span-495" class="mrow"><span id="MathJax-Span-496" class="mstyle"><span id="MathJax-Span-497" class="mrow"><span id="MathJax-Span-498" class="mi">t</span></span></span></span></span></span></span></span></span>&nbsp;should be, how "related" two words are is entirely determined by the task. In most cases, the definite article&nbsp;<span class="bc">the</span>&nbsp;is not very relevant to the interpretation of the other words in the sentence; therefore, we will likely end up with an embedding&nbsp;<span id="MathJax-Element-24-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-499" class="math"><span id="MathJax-Span-500" class="mrow"><span id="MathJax-Span-501" class="msubsup"><span id="MathJax-Span-502" class="texatom"><span id="MathJax-Span-503" class="mrow"><span id="MathJax-Span-504" class="mi">𝐯</span></span></span><span id="MathJax-Span-505" class="texatom"><span id="MathJax-Span-506" class="mrow"><span id="MathJax-Span-507" class="mstyle"><span id="MathJax-Span-508" class="mrow"><span id="MathJax-Span-509" class="mtext">the</span></span></span></span></span></span></span></span></span>&nbsp;that has a low or negative dot product with all other words. On the other hand, to interpret what&nbsp;<span class="bc">walks</span>&nbsp;means in this sentence, it's very helpful to work out&nbsp;<em>who</em>&nbsp;is doing the walking. This is likely expressed by a noun, so for nouns like&nbsp;<span class="bc">cat</span>&nbsp;and verbs like&nbsp;<span class="bc">walks</span>, we will likely learn embeddings&nbsp;<span id="MathJax-Element-25-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-510" class="math"><span id="MathJax-Span-511" class="mrow"><span id="MathJax-Span-512" class="msubsup"><span id="MathJax-Span-513" class="texatom"><span id="MathJax-Span-514" class="mrow"><span id="MathJax-Span-515" class="mi">𝐯</span></span></span><span id="MathJax-Span-516" class="texatom"><span id="MathJax-Span-517" class="mrow"><span id="MathJax-Span-518" class="mstyle"><span id="MathJax-Span-519" class="mrow"><span id="MathJax-Span-520" class="mtext">cat</span></span></span></span></span></span></span></span></span>&nbsp;and&nbsp;<span id="MathJax-Element-26-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-521" class="math"><span id="MathJax-Span-522" class="mrow"><span id="MathJax-Span-523" class="msubsup"><span id="MathJax-Span-524" class="texatom"><span id="MathJax-Span-525" class="mrow"><span id="MathJax-Span-526" class="mi">𝐯</span></span></span><span id="MathJax-Span-527" class="texatom"><span id="MathJax-Span-528" class="mrow"><span id="MathJax-Span-529" class="mstyle"><span id="MathJax-Span-530" class="mrow"><span id="MathJax-Span-531" class="mtext">walks</span></span></span></span></span></span></span></span></span>&nbsp;that have a high, positive dot product together.</p>
<p>This is the basic intuition behind self-attention. The dot product expresses how related two vectors in the input sequence are, with &ldquo;related&rdquo; defined by the learning task, and the output vectors are weighted sums over the whole input sequence, with the weights determined by these dot products.</p>
<p>Before we move on, it&rsquo;s worthwhile to note the following properties, which are unusual for a sequence-to-sequence operation:</p>
<ul>
<li>There are no parameters (yet). What the basic self-attention actually does is entirely determined by whatever mechanism creates the input sequence. Upstream mechanisms, like an embedding layer, drive the self-attention by learning representations with particular dot products (although we&rsquo;ll add a few parameters later).</li>
<li>Self attention sees its input as a&nbsp;<em>set</em>, not a sequence. If we permute the input sequence, the output sequence will be exactly the same, except permuted also (i.e. self-attention is&nbsp;<em>permutation equivariant</em>). We will mitigate this somewhat when we build the full transformer, but the self-attention by itself actually&nbsp;<em>ignores</em>&nbsp;the sequential nature of the input.</li>
</ul>
<h3 id="in-pytorch-basic-self-attention">In Pytorch: basic self-attention</h3>
<p>What I cannot create, I do not understand, as Feynman said. So we&rsquo;ll build a simple transformer as we go along. We&rsquo;ll start by implementing this basic self-attention operation in Pytorch.</p>
<p>The first thing we should do is work out how to express the self attention in matrix multiplications. A naive implementation that loops over all vectors to compute the weights and outputs would be much too slow.</p>
<p>We&rsquo;ll represent the input, a sequence of&nbsp;<span id="MathJax-Element-27-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-532" class="math"><span id="MathJax-Span-533" class="mrow"><span id="MathJax-Span-534" class="mi">t</span></span></span></span>&nbsp;vectors of dimension&nbsp;<span id="MathJax-Element-28-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-535" class="math"><span id="MathJax-Span-536" class="mrow"><span id="MathJax-Span-537" class="mi">k</span></span></span></span>&nbsp;as a&nbsp;<span id="MathJax-Element-29-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-538" class="math"><span id="MathJax-Span-539" class="mrow"><span id="MathJax-Span-540" class="mi">t</span></span></span></span>&nbsp;by&nbsp;<span id="MathJax-Element-30-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-541" class="math"><span id="MathJax-Span-542" class="mrow"><span id="MathJax-Span-543" class="mi">k</span></span></span></span>&nbsp;matrix&nbsp;<span id="MathJax-Element-31-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-544" class="math"><span id="MathJax-Span-545" class="mrow"><span id="MathJax-Span-546" class="texatom"><span id="MathJax-Span-547" class="mrow"><span id="MathJax-Span-548" class="mi">𝐗</span></span></span></span></span></span>. Including a minibatch dimension&nbsp;<span id="MathJax-Element-32-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-549" class="math"><span id="MathJax-Span-550" class="mrow"><span id="MathJax-Span-551" class="mi">b</span></span></span></span>, gives us an input tensor of size&nbsp;<span id="MathJax-Element-33-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-552" class="math"><span id="MathJax-Span-553" class="mrow"><span id="MathJax-Span-554" class="mo">(</span><span id="MathJax-Span-555" class="mi">b</span><span id="MathJax-Span-556" class="mo">,</span><span id="MathJax-Span-557" class="mi">t</span><span id="MathJax-Span-558" class="mo">,</span><span id="MathJax-Span-559" class="mi">k</span><span id="MathJax-Span-560" class="mo">)</span></span></span></span>.</p>
<p>The set of all raw dot products&nbsp;<span id="MathJax-Element-34-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-561" class="math"><span id="MathJax-Span-562" class="mrow"><span id="MathJax-Span-563" class="msubsup"><span id="MathJax-Span-564" class="mi">w</span><span id="MathJax-Span-565" class="mo">&prime;</span><span id="MathJax-Span-566" class="texatom"><span id="MathJax-Span-567" class="mrow"><span id="MathJax-Span-568" class="texatom"><span id="MathJax-Span-569" class="mrow"><span id="MathJax-Span-570" class="mstyle"><span id="MathJax-Span-571" class="mrow"><span id="MathJax-Span-572" class="mi">i</span></span></span></span></span><span id="MathJax-Span-573" class="texatom"><span id="MathJax-Span-574" class="mrow"><span id="MathJax-Span-575" class="mstyle"><span id="MathJax-Span-576" class="mrow"><span id="MathJax-Span-577" class="mi">j</span></span></span></span></span></span></span></span></span></span></span>&nbsp;forms a matrix, which we can compute simply by multiplying&nbsp;<span id="MathJax-Element-35-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-578" class="math"><span id="MathJax-Span-579" class="mrow"><span id="MathJax-Span-580" class="texatom"><span id="MathJax-Span-581" class="mrow"><span id="MathJax-Span-582" class="mi">𝐗</span></span></span></span></span></span>&nbsp;by its transpose:</p>
<pre><code class="hljs makefile">import torch
import torch.nn.functional as F

<span class="hljs-comment"># assume we have some tensor x with size (b, t, k)</span>
x = ...

raw_weights = torch.bmm(x, x.transpose(1, 2))
<span class="hljs-comment"># - torch.bmm is a batched matrix multiplication. It</span>
<span class="hljs-comment">#   applies matrix multiplication over batches of</span>
<span class="hljs-comment">#   matrices.</span></code></pre>
<p>Then, to turn the raw weights&nbsp;<span id="MathJax-Element-36-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-583" class="math"><span id="MathJax-Span-584" class="mrow"><span id="MathJax-Span-585" class="msubsup"><span id="MathJax-Span-586" class="mi">w</span><span id="MathJax-Span-587" class="mo">&prime;</span><span id="MathJax-Span-588" class="texatom"><span id="MathJax-Span-589" class="mrow"><span id="MathJax-Span-590" class="texatom"><span id="MathJax-Span-591" class="mrow"><span id="MathJax-Span-592" class="mstyle"><span id="MathJax-Span-593" class="mrow"><span id="MathJax-Span-594" class="mi">i</span></span></span></span></span><span id="MathJax-Span-595" class="texatom"><span id="MathJax-Span-596" class="mrow"><span id="MathJax-Span-597" class="mstyle"><span id="MathJax-Span-598" class="mrow"><span id="MathJax-Span-599" class="mi">j</span></span></span></span></span></span></span></span></span></span></span>&nbsp;into positive values that sum to one, we apply a&nbsp;<em>row-wise</em>&nbsp;softmax:</p>
<pre><code class="hljs ini"><span class="hljs-attr">weights</span> = F.softmax(raw_weights, dim=<span class="hljs-number">2</span>)</code></pre>
<p>Finally, to compute the output sequence, we just multiply the weight matrix by&nbsp;<span id="MathJax-Element-37-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-600" class="math"><span id="MathJax-Span-601" class="mrow"><span id="MathJax-Span-602" class="texatom"><span id="MathJax-Span-603" class="mrow"><span id="MathJax-Span-604" class="mi">𝐗</span></span></span></span></span></span>. This results in a batch of output matrices&nbsp;<span id="MathJax-Element-38-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-605" class="math"><span id="MathJax-Span-606" class="mrow"><span id="MathJax-Span-607" class="texatom"><span id="MathJax-Span-608" class="mrow"><span id="MathJax-Span-609" class="mi">𝐘</span></span></span></span></span></span>&nbsp;of size&nbsp;<code>(b, t, k)</code>&nbsp;whose rows are weighted sums over the rows of&nbsp;<span id="MathJax-Element-39-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-610" class="math"><span id="MathJax-Span-611" class="mrow"><span id="MathJax-Span-612" class="texatom"><span id="MathJax-Span-613" class="mrow"><span id="MathJax-Span-614" class="mi">𝐗</span></span></span></span></span></span>.</p>
<pre><code class="hljs ini"><span class="hljs-attr">y</span> = torch.bmm(weights, x)</code></pre>
<p>That&rsquo;s all. Two matrix multiplications and one softmax gives us a basic self-attention.</p>
<h3 id="additional-tricks">Additional tricks</h3>
<p>The actual self-attention used in modern transformers relies on three additional tricks.</p>
<h4 id="1-queries-keys-and-values">1) Queries, keys and values</h4>
<p>Every input vector&nbsp;<span id="MathJax-Element-40-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-615" class="math"><span id="MathJax-Span-616" class="mrow"><span id="MathJax-Span-617" class="msubsup"><span id="MathJax-Span-618" class="texatom"><span id="MathJax-Span-619" class="mrow"><span id="MathJax-Span-620" class="mi">𝐱</span></span></span><span id="MathJax-Span-621" class="texatom"><span id="MathJax-Span-622" class="mrow"><span id="MathJax-Span-623" class="mstyle"><span id="MathJax-Span-624" class="mrow"><span id="MathJax-Span-625" class="mi">i</span></span></span></span></span></span></span></span></span>&nbsp;is used in three different ways in the self attention operation:</p>
<ul>
<li>It is compared to every other vector to establish the weights for its own output&nbsp;<span id="MathJax-Element-41-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-626" class="math"><span id="MathJax-Span-627" class="mrow"><span id="MathJax-Span-628" class="msubsup"><span id="MathJax-Span-629" class="texatom"><span id="MathJax-Span-630" class="mrow"><span id="MathJax-Span-631" class="mi">𝐲</span></span></span><span id="MathJax-Span-632" class="texatom"><span id="MathJax-Span-633" class="mrow"><span id="MathJax-Span-634" class="mstyle"><span id="MathJax-Span-635" class="mrow"><span id="MathJax-Span-636" class="mi">i</span></span></span></span></span></span></span></span></span></li>
<li>It is compared to every other vector to establish the weights for the output of the&nbsp;<span id="MathJax-Element-42-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-637" class="math"><span id="MathJax-Span-638" class="mrow"><span id="MathJax-Span-639" class="texatom"><span id="MathJax-Span-640" class="mrow"><span id="MathJax-Span-641" class="mstyle"><span id="MathJax-Span-642" class="mrow"><span id="MathJax-Span-643" class="mi">j</span></span></span></span></span></span></span></span>-th vector&nbsp;<span id="MathJax-Element-43-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-644" class="math"><span id="MathJax-Span-645" class="mrow"><span id="MathJax-Span-646" class="msubsup"><span id="MathJax-Span-647" class="texatom"><span id="MathJax-Span-648" class="mrow"><span id="MathJax-Span-649" class="mi">𝐲</span></span></span><span id="MathJax-Span-650" class="texatom"><span id="MathJax-Span-651" class="mrow"><span id="MathJax-Span-652" class="mstyle"><span id="MathJax-Span-653" class="mrow"><span id="MathJax-Span-654" class="mi">j</span></span></span></span></span></span></span></span></span></li>
<li>It is used as part of the weighted sum to compute each output vector once the weights have been established.</li>
</ul>
<p>These roles are often called the&nbsp;<strong>query</strong>, the&nbsp;<strong>key</strong>&nbsp;and the&nbsp;<strong>value</strong>&nbsp;(we'll explain where these names come from later). In the basic self-attention we've seen so far, each input vector must play all three roles. We make its life a little easier by deriving new vectors for each role, by applying a linear transformation to the original input vector. In other words, we add three&nbsp;<span id="MathJax-Element-44-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-655" class="math"><span id="MathJax-Span-656" class="mrow"><span id="MathJax-Span-657" class="mi">k</span><span id="MathJax-Span-658" class="mo">&times;</span><span id="MathJax-Span-659" class="mi">k</span></span></span></span>&nbsp;weight matrices&nbsp;<span id="MathJax-Element-45-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-660" class="math"><span id="MathJax-Span-661" class="mrow"><span id="MathJax-Span-662" class="msubsup"><span id="MathJax-Span-663" class="texatom"><span id="MathJax-Span-664" class="mrow"><span id="MathJax-Span-665" class="mi">𝐖</span></span></span><span id="MathJax-Span-666" class="mi">q</span></span></span></span></span>,&nbsp;<span id="MathJax-Element-46-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-667" class="math"><span id="MathJax-Span-668" class="mrow"><span id="MathJax-Span-669" class="msubsup"><span id="MathJax-Span-670" class="texatom"><span id="MathJax-Span-671" class="mrow"><span id="MathJax-Span-672" class="mi">𝐖</span></span></span><span id="MathJax-Span-673" class="mi">k</span></span></span></span></span>,<span id="MathJax-Element-47-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-674" class="math"><span id="MathJax-Span-675" class="mrow"><span id="MathJax-Span-676" class="msubsup"><span id="MathJax-Span-677" class="texatom"><span id="MathJax-Span-678" class="mrow"><span id="MathJax-Span-679" class="mi">𝐖</span></span></span><span id="MathJax-Span-680" class="mi">v</span></span></span></span></span>&nbsp;and compute three linear transformations of each&nbsp;<span id="MathJax-Element-48-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-681" class="math"><span id="MathJax-Span-682" class="mrow"><span id="MathJax-Span-683" class="msubsup"><span id="MathJax-Span-684" class="mi">x</span><span id="MathJax-Span-685" class="texatom"><span id="MathJax-Span-686" class="mrow"><span id="MathJax-Span-687" class="mstyle"><span id="MathJax-Span-688" class="mrow"><span id="MathJax-Span-689" class="mi">i</span></span></span></span></span></span></span></span></span>, for the three different parts of the self attention:</p>
<div class="MathJax_Display"><span id="MathJax-Element-49-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-690" class="math"><span id="MathJax-Span-691" class="mrow"><span id="MathJax-Span-692" class="mtable"><span id="MathJax-Span-693" class="mtd"><span id="MathJax-Span-694" class="mrow"><span id="MathJax-Span-695" class="msubsup"><span id="MathJax-Span-696" class="texatom"><span id="MathJax-Span-697" class="mrow"><span id="MathJax-Span-698" class="mi">𝐪</span></span></span><span id="MathJax-Span-699" class="texatom"><span id="MathJax-Span-700" class="mrow"><span id="MathJax-Span-701" class="mstyle"><span id="MathJax-Span-702" class="mrow"><span id="MathJax-Span-703" class="mi">i</span></span></span></span></span></span></span></span><span id="MathJax-Span-704" class="mtd"><span id="MathJax-Span-705" class="mrow"><span id="MathJax-Span-706" class="mi"></span><span id="MathJax-Span-707" class="mo">=</span><span id="MathJax-Span-708" class="msubsup"><span id="MathJax-Span-709" class="texatom"><span id="MathJax-Span-710" class="mrow"><span id="MathJax-Span-711" class="mi">𝐖</span></span></span><span id="MathJax-Span-712" class="mi">q</span></span><span id="MathJax-Span-713" class="msubsup"><span id="MathJax-Span-714" class="texatom"><span id="MathJax-Span-715" class="mrow"><span id="MathJax-Span-716" class="mi">𝐱</span></span></span><span id="MathJax-Span-717" class="texatom"><span id="MathJax-Span-718" class="mrow"><span id="MathJax-Span-719" class="mstyle"><span id="MathJax-Span-720" class="mrow"><span id="MathJax-Span-721" class="mi">i</span></span></span></span></span></span></span></span><span id="MathJax-Span-722" class="mtd"><span id="MathJax-Span-723" class="mrow"><span id="MathJax-Span-724" class="msubsup"><span id="MathJax-Span-725" class="texatom"><span id="MathJax-Span-726" class="mrow"><span id="MathJax-Span-727" class="mi">𝐤</span></span></span><span id="MathJax-Span-728" class="texatom"><span id="MathJax-Span-729" class="mrow"><span id="MathJax-Span-730" class="mstyle"><span id="MathJax-Span-731" class="mrow"><span id="MathJax-Span-732" class="mi">i</span></span></span></span></span></span></span></span><span id="MathJax-Span-733" class="mtd"><span id="MathJax-Span-734" class="mrow"><span id="MathJax-Span-735" class="mi"></span><span id="MathJax-Span-736" class="mo">=</span><span id="MathJax-Span-737" class="msubsup"><span id="MathJax-Span-738" class="texatom"><span id="MathJax-Span-739" class="mrow"><span id="MathJax-Span-740" class="mi">𝐖</span></span></span><span id="MathJax-Span-741" class="mi">k</span></span><span id="MathJax-Span-742" class="msubsup"><span id="MathJax-Span-743" class="texatom"><span id="MathJax-Span-744" class="mrow"><span id="MathJax-Span-745" class="mi">𝐱</span></span></span><span id="MathJax-Span-746" class="texatom"><span id="MathJax-Span-747" class="mrow"><span id="MathJax-Span-748" class="mstyle"><span id="MathJax-Span-749" class="mrow"><span id="MathJax-Span-750" class="mi">i</span></span></span></span></span></span></span></span><span id="MathJax-Span-751" class="mtd"><span id="MathJax-Span-752" class="mrow"><span id="MathJax-Span-753" class="msubsup"><span id="MathJax-Span-754" class="texatom"><span id="MathJax-Span-755" class="mrow"><span id="MathJax-Span-756" class="mi">𝐯</span></span></span><span id="MathJax-Span-757" class="texatom"><span id="MathJax-Span-758" class="mrow"><span id="MathJax-Span-759" class="mstyle"><span id="MathJax-Span-760" class="mrow"><span id="MathJax-Span-761" class="mi">i</span></span></span></span></span></span></span></span><span id="MathJax-Span-762" class="mtd"><span id="MathJax-Span-763" class="mrow"><span id="MathJax-Span-764" class="mi"></span><span id="MathJax-Span-765" class="mo">=</span><span id="MathJax-Span-766" class="msubsup"><span id="MathJax-Span-767" class="texatom"><span id="MathJax-Span-768" class="mrow"><span id="MathJax-Span-769" class="mi">𝐖</span></span></span><span id="MathJax-Span-770" class="mi">v</span></span><span id="MathJax-Span-771" class="msubsup"><span id="MathJax-Span-772" class="texatom"><span id="MathJax-Span-773" class="mrow"><span id="MathJax-Span-774" class="mi">𝐱</span></span></span><span id="MathJax-Span-775" class="texatom"><span id="MathJax-Span-776" class="mrow"><span id="MathJax-Span-777" class="mstyle"><span id="MathJax-Span-778" class="mrow"><span id="MathJax-Span-779" class="mi">i</span></span></span></span></span></span></span></span></span></span></span></span></div>
<div class="MathJax_Display"><span id="MathJax-Element-50-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-780" class="math"><span id="MathJax-Span-781" class="mrow"><span id="MathJax-Span-782" class="mtable"><span id="MathJax-Span-783" class="mtd"><span id="MathJax-Span-784" class="mrow"><span id="MathJax-Span-785" class="msubsup"><span id="MathJax-Span-786" class="mi">w</span><span id="MathJax-Span-787" class="mo">&prime;</span><span id="MathJax-Span-788" class="texatom"><span id="MathJax-Span-789" class="mrow"><span id="MathJax-Span-790" class="texatom"><span id="MathJax-Span-791" class="mrow"><span id="MathJax-Span-792" class="mstyle"><span id="MathJax-Span-793" class="mrow"><span id="MathJax-Span-794" class="mi">i</span></span></span></span></span><span id="MathJax-Span-795" class="texatom"><span id="MathJax-Span-796" class="mrow"><span id="MathJax-Span-797" class="mstyle"><span id="MathJax-Span-798" class="mrow"><span id="MathJax-Span-799" class="mi">j</span></span></span></span></span></span></span></span></span></span><span id="MathJax-Span-826" class="mtd"><span id="MathJax-Span-827" class="mrow"><span id="MathJax-Span-828" class="msubsup"><span id="MathJax-Span-829" class="mi">w</span><span id="MathJax-Span-830" class="texatom"><span id="MathJax-Span-831" class="mrow"><span id="MathJax-Span-832" class="texatom"><span id="MathJax-Span-833" class="mrow"><span id="MathJax-Span-834" class="mstyle"><span id="MathJax-Span-835" class="mrow"><span id="MathJax-Span-836" class="mi">i</span></span></span></span></span><span id="MathJax-Span-837" class="texatom"><span id="MathJax-Span-838" class="mrow"><span id="MathJax-Span-839" class="mstyle"><span id="MathJax-Span-840" class="mrow"><span id="MathJax-Span-841" class="mi">j</span></span></span></span></span></span></span></span></span></span><span id="MathJax-Span-864" class="mtd"><span id="MathJax-Span-865" class="mrow"><span id="MathJax-Span-866" class="msubsup"><span id="MathJax-Span-867" class="texatom"><span id="MathJax-Span-868" class="mrow"><span id="MathJax-Span-869" class="mi">𝐲</span></span></span><span id="MathJax-Span-870" class="texatom"><span id="MathJax-Span-871" class="mrow"><span id="MathJax-Span-872" class="mstyle"><span id="MathJax-Span-873" class="mrow"><span id="MathJax-Span-874" class="mi">i</span></span></span></span></span></span></span></span><span id="MathJax-Span-800" class="mtd"><span id="MathJax-Span-801" class="mrow"><span id="MathJax-Span-802" class="mi"></span><span id="MathJax-Span-803" class="mo">=</span><span id="MathJax-Span-804" class="msubsup"><span id="MathJax-Span-805" class="texatom"><span id="MathJax-Span-806" class="mrow"><span id="MathJax-Span-807" class="msubsup"><span id="MathJax-Span-808" class="texatom"><span id="MathJax-Span-809" class="mrow"><span id="MathJax-Span-810" class="mi">𝐪</span></span></span><span id="MathJax-Span-811" class="texatom"><span id="MathJax-Span-812" class="mrow"><span id="MathJax-Span-813" class="mstyle"><span id="MathJax-Span-814" class="mrow"><span id="MathJax-Span-815" class="mi">i</span></span></span></span></span></span></span></span><span id="MathJax-Span-816" class="mi">T</span></span><span id="MathJax-Span-817" class="msubsup"><span id="MathJax-Span-818" class="texatom"><span id="MathJax-Span-819" class="mrow"><span id="MathJax-Span-820" class="mi">𝐤</span></span></span><span id="MathJax-Span-821" class="texatom"><span id="MathJax-Span-822" class="mrow"><span id="MathJax-Span-823" class="mstyle"><span id="MathJax-Span-824" class="mrow"><span id="MathJax-Span-825" class="mi">j</span></span></span></span></span></span></span></span><span id="MathJax-Span-842" class="mtd"><span id="MathJax-Span-843" class="mrow"><span id="MathJax-Span-844" class="mi"></span><span id="MathJax-Span-845" class="mo">=</span><span id="MathJax-Span-846" class="mtext">softmax</span><span id="MathJax-Span-847" class="mo">(</span><span id="MathJax-Span-848" class="msubsup"><span id="MathJax-Span-849" class="mi">w</span><span id="MathJax-Span-850" class="mo">&prime;</span><span id="MathJax-Span-851" class="texatom"><span id="MathJax-Span-852" class="mrow"><span id="MathJax-Span-853" class="texatom"><span id="MathJax-Span-854" class="mrow"><span id="MathJax-Span-855" class="mstyle"><span id="MathJax-Span-856" class="mrow"><span id="MathJax-Span-857" class="mi">i</span></span></span></span></span><span id="MathJax-Span-858" class="texatom"><span id="MathJax-Span-859" class="mrow"><span id="MathJax-Span-860" class="mstyle"><span id="MathJax-Span-861" class="mrow"><span id="MathJax-Span-862" class="mi">j</span></span></span></span></span></span></span></span><span id="MathJax-Span-863" class="mo">)</span></span></span><span id="MathJax-Span-875" class="mtd"><span id="MathJax-Span-876" class="mrow"><span id="MathJax-Span-877" class="mi"></span><span id="MathJax-Span-878" class="mo">=</span><span id="MathJax-Span-879" class="munderover"><span id="MathJax-Span-880" class="mo">&sum;</span><span id="MathJax-Span-881" class="texatom"><span id="MathJax-Span-882" class="mrow"><span id="MathJax-Span-883" class="mstyle"><span id="MathJax-Span-884" class="mrow"><span id="MathJax-Span-885" class="mi">j</span></span></span></span></span></span><span id="MathJax-Span-886" class="msubsup"><span id="MathJax-Span-887" class="mi">w</span><span id="MathJax-Span-888" class="texatom"><span id="MathJax-Span-889" class="mrow"><span id="MathJax-Span-890" class="texatom"><span id="MathJax-Span-891" class="mrow"><span id="MathJax-Span-892" class="mstyle"><span id="MathJax-Span-893" class="mrow"><span id="MathJax-Span-894" class="mi">i</span></span></span></span></span><span id="MathJax-Span-895" class="texatom"><span id="MathJax-Span-896" class="mrow"><span id="MathJax-Span-897" class="mstyle"><span id="MathJax-Span-898" class="mrow"><span id="MathJax-Span-899" class="mi">j</span></span></span></span></span></span></span></span><span id="MathJax-Span-900" class="msubsup"><span id="MathJax-Span-901" class="texatom"><span id="MathJax-Span-902" class="mrow"><span id="MathJax-Span-903" class="mi">𝐯</span></span></span><span id="MathJax-Span-904" class="texatom"><span id="MathJax-Span-905" class="mrow"><span id="MathJax-Span-906" class="mstyle"><span id="MathJax-Span-907" class="mrow"><span id="MathJax-Span-908" class="mi">j</span></span></span></span></span></span><span id="MathJax-Span-909" class="mspace"></span><span id="MathJax-Span-910" class="mtext">.</span></span></span></span></span></span></span></div>
<p>This gives the self-attention layer some controllable parameters, and allows it to modify the incoming vectors to suit the three roles they must play.</p>
<figure class="narrow"><img src="https://peterbloem.nl/files/transformers/key-query-value.svg" alt="" />
<figcaption>Illustration of the self-attention with&nbsp;<span class="bc">key</span>,&nbsp;<span class="rc">query</span>&nbsp;and&nbsp;<span class="gc">value</span>&nbsp;transformations.</figcaption>
</figure>
<h4 id="2-scaling-the-dot-product">2) Scaling the dot product</h4>
<p>The softmax function can be sensitive to very large input values. These kill the gradient, and slow down learning, or cause it to stop altogether. Since the average value of the dot product grows with the embedding dimension&nbsp;<span id="MathJax-Element-51-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-911" class="math"><span id="MathJax-Span-912" class="mrow"><span id="MathJax-Span-913" class="mi">k</span></span></span></span>, it helps to scale the dot product back a little to stop the inputs to the softmax function from growing too large:</p>
<div class="MathJax_Display"><span id="MathJax-Element-52-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-914" class="math"><span id="MathJax-Span-915" class="mrow"><span id="MathJax-Span-916" class="msubsup"><span id="MathJax-Span-917" class="mi">w</span><span id="MathJax-Span-918" class="mo">&prime;</span><span id="MathJax-Span-919" class="texatom"><span id="MathJax-Span-920" class="mrow"><span id="MathJax-Span-921" class="texatom"><span id="MathJax-Span-922" class="mrow"><span id="MathJax-Span-923" class="mstyle"><span id="MathJax-Span-924" class="mrow"><span id="MathJax-Span-925" class="mi">i</span></span></span></span></span><span id="MathJax-Span-926" class="texatom"><span id="MathJax-Span-927" class="mrow"><span id="MathJax-Span-928" class="mstyle"><span id="MathJax-Span-929" class="mrow"><span id="MathJax-Span-930" class="mi">j</span></span></span></span></span></span></span></span><span id="MathJax-Span-931" class="mo">=</span><span id="MathJax-Span-932" class="mfrac"><span id="MathJax-Span-933" class="mrow"><span id="MathJax-Span-934" class="msubsup"><span id="MathJax-Span-935" class="texatom"><span id="MathJax-Span-936" class="mrow"><span id="MathJax-Span-937" class="msubsup"><span id="MathJax-Span-938" class="texatom"><span id="MathJax-Span-939" class="mrow"><span id="MathJax-Span-940" class="mi">𝐪</span></span></span><span id="MathJax-Span-941" class="texatom"><span id="MathJax-Span-942" class="mrow"><span id="MathJax-Span-943" class="mstyle"><span id="MathJax-Span-944" class="mrow"><span id="MathJax-Span-945" class="mi">i</span></span></span></span></span></span></span></span><span id="MathJax-Span-946" class="mi">T</span></span><span id="MathJax-Span-947" class="msubsup"><span id="MathJax-Span-948" class="texatom"><span id="MathJax-Span-949" class="mrow"><span id="MathJax-Span-950" class="mi">𝐤</span></span></span><span id="MathJax-Span-951" class="texatom"><span id="MathJax-Span-952" class="mrow"><span id="MathJax-Span-953" class="mstyle"><span id="MathJax-Span-954" class="mrow"><span id="MathJax-Span-955" class="mi">j</span></span></span></span></span></span></span><span id="MathJax-Span-956" class="msqrt"><span id="MathJax-Span-957" class="mrow"><span id="MathJax-Span-958" class="mi">k</span></span>&minus;&minus;&radic;</span></span></span></span></span></div>
<!-- Comments are visible in the HTML source only -->`
const BlogDetails = () => {
    return (

        <div className="py-10 px-5 lg:p-20 text-left">
            <div dangerouslySetInnerHTML={{ __html: data}}/>
            
        </div>

    )
};

export default BlogDetails;